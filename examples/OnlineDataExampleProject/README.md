# OnlineDataExampleProject

A P.A.P.E.R (Platform for Asset-Pricing Experimentation and Research) project.

Initialized on: 2025-06-18
P.A.P.E.R Tools Version: 0.1.0

## Project Structure

This project follows the standard P.A.P.E.R directory structure:

- `configs/`: Contains all configuration files for the project.
    - `paper-project.yaml`: Main project configuration linking all components.
    - `data-config.yaml`: Defines the data ingestion and wrangling pipeline.
    - `models-config.yaml`: Defines the models to be trained and evaluated.
    - `portfolio-config.yaml`: Defines portfolio construction and backtesting strategies.
- `data/`: Contains all data-related assets.
    - `raw/`: Place your raw, original data files here (e.g., CSVs).
    - `processed/`: The destination for cleaned, analysis-ready datasets generated by the `paper-data` pipeline.
    - `scripts/`: Place custom Python transformation scripts here. These scripts can be called from your `data-config.yaml` to perform complex, project-specific data manipulations.
- `models/`: Contains all modeling-related outputs.
    - `predictions/`: Stores the out-of-sample predictions generated by each model.
    - `evaluations/`: Contains performance metrics (e.g., RÂ², MSE) for each model.
    - `saved/`: Stores saved model objects for each training window.
- `portfolios/`: Contains all portfolio analysis outputs.
    - `results/`: Stores performance reports, charts, and monthly returns for each backtested strategy.
    - `additional_datasets/`: Place for supplementary files needed for portfolio analysis (e.g., risk-free rates, benchmarks).
- `logs.log`: A central log file that captures detailed output from all pipeline phases.
- `.gitignore`: A pre-configured file to prevent temporary files and processed data from being committed to version control.
- `README.md`: This file.

## How to Use

1.  **Configure**: Edit the YAML files in the `configs/` directory to define your research pipeline.
2.  **Add Data**: Place any local raw data files in `data/raw/` and custom transformation scripts in `data/scripts/`.
3.  **Execute**: Run the pipeline phases sequentially from your terminal:
    ```bash
    paper execute data
    paper execute models
    paper execute portfolio
    ```
