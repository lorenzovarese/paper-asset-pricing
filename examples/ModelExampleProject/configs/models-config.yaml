# ---------------------------------------------------------------------------
# P.A.P.E.R Model Configuration File
# This file defines the entire modeling and evaluation pipeline.
# ---------------------------------------------------------------------------

# --- Global Input Data and Evaluation Settings ---

# Defines the source data and the structure of the backtest.
# These settings apply to all models listed below.

input_data:
  # The base name of the processed dataset generated by the paper-data component.
  dataset_name: "final_dataset_model"
  # Instructs the system that the data is stored in yearly partitioned Parquet files.
  # This enables efficient, incremental loading for the rolling window.
  splitted: "year"
  # Key column identifiers used throughout the modeling process.
  date_column: "date"
  id_column: "permno"
  risk_free_rate_col: "rf"

evaluation:
  # The backtesting methodology. "rolling window" is the standard for time-series.
  implementation: "rolling window"
  # --- Rolling Window Structure ---
  # For each window, the model will be trained on 10 years of historical data.
  train_month: 120
  # A 2-year gap is used for hyperparameter tuning. This data is not used for final model training.
  validation_month: 24
  # The trained model is then evaluated on the subsequent 12 months, one by one.
  testing_month: 12
  # After a full cycle, the entire window (train + validation + test) slides forward by 1 year.
  step_month: 12
  # List of predictive performance metrics to calculate for each model.
  metrics: [ "mse", "r2_oos", "r2_adj_oos" ]

# --- Model Definitions ---

# A list of all models to be trained and evaluated in the pipeline.
models:

# --- Model 1: Ordinary Least Squares (OLS) with Robust Loss ---
- name: "ols_huber_adaptive"
  type: "ols" # Specifies the model type.
  target_column: "ret" # The variable to be predicted.
  # A simple baseline model using only the three Fama-French factors.
  features:
  - "bm" # Book-to-Market
  - "mvel1" # Market Value (Size)
  - "mom12m" # 12-Month Momentum
  # Use the Huber loss function instead of standard least squares (L2) to be robust to outliers.
  objective_function: "huber"
  # An advanced feature: instead of a fixed Huber threshold (delta), adaptively set it
  # to the 99.9th percentile of the absolute errors from a preliminary OLS fit.
  # This makes the model's robustness data-driven for each training window.
  huber_epsilon_quantile: 0.999

# --- Model 2: Elastic Net (ENet) with Fixed Parameters ---
- name: "elastic_net_model"
  type: "enet"
  target_column: "ret"
  # Use a wider set of 20 predictive characteristics.
  features: [ "bm", "mvel1", "mom12m", "beta", "sp", "securedind", "mom36m", "zerotrade", "nincr", "indmom", "std_turn", "ill", "baspread", "mom6m", "idiovol", "turn", "dolvol", "chmom", "maxret", "retvol" ]
  # --- Regularization Parameters (Fixed) ---
  # alpha (α): Overall regularization strength.
  alpha: 0.1
  # l1_ratio (ρ): The mix between L1 (Lasso) and L2 (Ridge) penalties. 0.5 is an equal mix.
  l1_ratio: 0.5
  random_state: 306 # For reproducibility.
  save_model_checkpoints: true # Save the trained model object for each window.
  save_prediction_results: true # Save the out-of-sample predictions to a file.

# --- Model 3: Principal Component Regression (PCR) with Tuning ---
- name: "pcr_tuned_model"
  type: "pcr" # A two-step model: PCA for dimensionality reduction, then OLS.
  target_column: "ret"
  features: [ "bm", "mvel1", "mom12m", "beta", "sp", "securedind", "mom36m", "zerotrade", "nincr", "indmom", "std_turn", "ill", "baspread", "mom6m", "idiovol", "turn", "dolvol", "chmom", "maxret", "retvol" ]
  # --- Hyperparameter Tuning ---
  # Instead of a fixed number of components, provide a list. The system will use the
  # validation set to find the best number of components (K) to retain for each window.
  n_components: [ 5, 10, 15 ]
  # Use the robust Huber loss for the final regression step on the principal components.
  objective_function: "huber"
  random_state: 306
  save_model_checkpoints: true
  save_prediction_results: true

# --- Model 4: Partial Least Squares (PLS) with Tuning ---
- name: "pls_tuned_model"
  type: "pls" # A supervised dimensionality reduction technique.
  target_column: "ret"
  features: [ "bm", "mvel1", "mom12m", "beta", "sp", "securedind", "mom36m", "zerotrade", "nincr", "indmom", "std_turn", "ill", "baspread", "mom6m", "idiovol", "turn", "dolvol", "chmom", "maxret", "retvol" ]
  # Tune the number of components (K) to find the optimal predictive dimensionality.
  n_components: [ 3, 6, 9 ]
  random_state: 306
  save_model_checkpoints: true
  save_prediction_results: true

# --- Model 5: Generalized Linear Model (GLM) with Splines and Group Lasso Tuning ---
- name: "glm_tuned_model"
  type: "glm"
  target_column: "ret"
  features: [ "bm", "mvel1", "mom12m", "beta", "sp" ]
  # Use 3 knots to create quadratic splines, allowing the model to capture non-linearities.
  n_knots: 3
  # Tune the Group Lasso regularization strength (λ) over the specified grid.
  # This will perform feature selection at the level of the original characteristics.
  alpha: [ 0.0001, 0.001, 0.01, 0.1 ]
  # The underlying solver for Group Lasso requires a standard L2 objective.
  objective_function: "l2"
  random_state: 306
  save_model_checkpoints: true
  save_prediction_results: true

# --- Model 6: Random Forest (RF) with Tuning ---
- name: "random_forest_tuned"
  type: "rf"
  target_column: "ret"
  features: [ "bm", "mvel1", "mom12m", "beta", "sp" ]
  # The number of trees (B) in the forest is fixed.
  n_estimators: 300
  # --- Hyperparameter Tuning ---
  # Tune the maximum depth of each tree (L).
  max_depth: [ 1, 2, 3, 4, 5, 6 ]
  # Tune the number of features to consider at each split.
  # "sqrt" uses sqrt(P) features; 0.33 uses 33% of features, etc.
  max_features: [ "sqrt", 0.33, 0.5 ]
  random_state: 306
  save_model_checkpoints: true
  save_prediction_results: true

# --- Model 7: Gradient Boosted Regression Tree (GBRT) with Tuning ---
- name: "gbrt_huber_tuned"
  type: "gbrt"
  target_column: "ret"
  features: [ "bm", "mvel1", "mom12m", "beta", "sp" ]
  # Use the robust Huber loss for the sequential tree fitting.
  objective_function: "huber"
  # --- Hyperparameter Tuning ---
  # Tune the number of sequential trees (B), the max depth of each tree (L),
  # and the learning rate (ν) which controls the contribution of each tree.
  n_estimators: [ 100, 500, 1000 ]
  max_depth: [ 1, 2 ]
  learning_rate: [ 0.01, 0.1 ]
  random_state: 306
  save_model_checkpoints: true
  save_prediction_results: true

# --- Model 8: Neural Network (NN1 - Shallow) with Tuning ---
- name: "nn1_model"
  type: "nn"
  target_column: "ret"
  features: [ "bm", "mvel1", "mom12m", "beta", "sp", "securedind", "mom36m", "zerotrade", "nincr", "indmom", "std_turn", "ill", "baspread", "mom6m", "idiovol", "turn", "dolvol", "chmom", "maxret", "retvol" ]
  # Defines the architecture: a single hidden layer with 32 neurons.
  hidden_layer_sizes: [ 32 ]
  # --- Hyperparameter Tuning ---
  # Tune the L1 regularization penalty (λ).
  alpha: [ 0.00001, 0.0001 ]
  # Tune the learning rate for the Adam optimizer.
  learning_rate: [ 0.001, 0.01 ]
  # --- Training Parameters ---
  batch_size: 10000 # Number of samples per gradient update.
  epochs: 100 # Maximum number of passes through the training data.
  patience: 5 # Stop training if validation loss doesn't improve for 5 epochs.
  n_ensembles: 10 # Train 10 identical networks and average their predictions for stability.
  random_state: 306
  save_model_checkpoints: true
  save_prediction_results: true

# --- Model 9: Neural Network (NN3 - Deeper) with Tuning ---
- name: "nn3_model"
  type: "nn"
  target_column: "ret"
  features: [ "bm", "mvel1", "mom12m", "beta", "sp", "securedind", "mom36m", "zerotrade", "nincr", "indmom", "std_turn", "ill", "baspread", "mom6m", "idiovol", "turn", "dolvol", "chmom", "maxret", "retvol" ]
  # Defines a deeper architecture: three hidden layers with 32, 16, and 8 neurons.
  hidden_layer_sizes: [ 32, 16, 8 ]
  # Tune the same hyperparameters as the shallow network.
  alpha: [ 0.00001, 0.0001 ]
  learning_rate: [ 0.001, 0.01 ]
  # Use the same training parameters.
  batch_size: 10000
  epochs: 100
  patience: 5
  n_ensembles: 10
  random_state: 306
  save_model_checkpoints: true
  save_prediction_results: true
