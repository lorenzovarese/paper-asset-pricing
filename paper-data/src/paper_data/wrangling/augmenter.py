"""Data augmentation library for paper data."""

from typing import Literal
import polars as pl
import logging

logger = logging.getLogger(__name__)


def merge_datasets(
    left_df: pl.DataFrame,
    right_df: pl.DataFrame,
    on_cols: list[str],
    how: Literal["left", "inner", "outer", "full"] = "left",
) -> pl.DataFrame:
    """
    Merges two Polars DataFrames.

    Args:
        left_df: The left DataFrame.
        right_df: The right DataFrame.
        on_cols: A list of column names to merge on.
        how: The type of join (e.g., "left", "inner", "outer", "full").

    Returns:
        The merged Polars DataFrame.
    """
    logger.info(f"Merging datasets on columns: {on_cols} with how='{how}'")
    merged_df = left_df.join(right_df, on=on_cols, how=how)
    logger.info(f"Merge complete. Resulting shape: {merged_df.shape}")
    return merged_df


def lag_columns(
    df: pl.DataFrame,
    date_col: str,
    id_col: str | None,
    cols_to_lag: list[str],
    periods: int,
    drop_original_cols_after_lag: bool,
    restore_names: bool,
    drop_generated_nans: bool,
) -> pl.DataFrame:
    """
    Lags or leads specified columns in a Polars DataFrame.

    If an `id_col` is provided, the lagging is performed within each group
    defined by `id_col`, ordered by `date_col`. Otherwise, it's a simple
    time-series lag ordered by `date_col`.

    Args:
        df: The input Polars DataFrame.
        date_col: The name of the date column to order by.
        id_col: The name of the identifier column for panel data lagging (e.g., 'permco').
                If None, a simple time-series lag is performed.
        cols_to_lag: A list of column names to apply the lag/lead operation to.
        periods: The number of periods to shift.
                 Positive for lagging (e.g., 1 for previous period's value).
                 Negative for leading (e.g., -1 for next period's value).
        drop_original_cols_after_lag: If True, the original (unlagged/unled) columns are dropped
                                      from the output DataFrame.
        restore_names: If True, the lagged columns will be renamed back to their
                       original names (e.g., 'volume_lag_1' becomes 'volume').
                       This requires `drop_original_cols_after_lag` to be true.
        drop_generated_nans: If True, rows containing NaN values that were
                             generated by the lagging operation will be dropped.

    Returns:
        A new Polars DataFrame with the lagged/led columns.
    """
    if not cols_to_lag:
        logger.info("No columns specified for lagging. Returning original DataFrame.")
        return df

    if date_col not in df.columns:
        raise ValueError(f"Date column '{date_col}' not found in DataFrame.")
    if not isinstance(df[date_col].dtype, (pl.Date, pl.Datetime)):
        raise ValueError(
            f"Date column '{date_col}' must be of Date or Datetime type for lagging."
        )

    # Validate that all specified columns exist in the DataFrame
    missing_cols = [c for c in cols_to_lag if c not in df.columns]
    if missing_cols:
        raise ValueError(
            f"Columns specified for lagging not found in DataFrame: {missing_cols}"
        )

    out_df = df.clone()
    suffix = f"_lag_{periods}" if periods > 0 else f"_lead_{abs(periods)}"
    lagged_col_names = []  # Keep track of the new lagged column names

    logger.info(
        f"Applying {'lag' if periods > 0 else 'lead'} of {abs(periods)} periods to columns: {cols_to_lag}"
    )
    if not id_col:
        logger.info(f"Sorting DataFrame by '{date_col}' for time-series operations.")
        out_df = out_df.sort(date_col)

    expressions = []
    for col in cols_to_lag:
        lagged_col_name = f"{col}{suffix}"
        lagged_col_names.append(lagged_col_name)

        if id_col:
            expr = (
                pl.col(col)
                .sort_by(date_col)
                .shift(periods)
                .over(id_col)
                .alias(lagged_col_name)
            )
        else:
            expr = pl.col(col).shift(periods).alias(lagged_col_name)
        expressions.append(expr)

    out_df = out_df.with_columns(expressions)

    if id_col:
        logger.info(
            f"Sorting DataFrame by '{id_col}' and '{date_col}' for panel operations."
        )
        out_df = out_df.sort([id_col, date_col])
    else:
        logger.info(
            f"Sorting DataFrame by '{date_col}' for time-series operations (no identifier column)."
        )
        out_df = out_df.sort(date_col)

    out_df = out_df.with_columns(expressions)

    # Handle dropping generated NaNs
    if drop_generated_nans:
        logger.info(
            f"Dropping rows with NaN values generated by lagging in columns: {lagged_col_names}"
        )
        # Create a condition to check for nulls in any of the newly lagged columns
        # We only care about nulls in the *newly created* lagged columns
        null_check_expressions = [
            pl.col(col_name).is_null()
            for col_name in lagged_col_names
            if col_name in out_df.columns
        ]

        if null_check_expressions:  # Only filter if there are actual columns to check
            initial_rows = out_df.shape[0]
            # Combine all null checks with an OR condition
            combined_null_condition = null_check_expressions[0]
            for i in range(1, len(null_check_expressions)):
                combined_null_condition = (
                    combined_null_condition | null_check_expressions[i]
                )

            out_df = out_df.filter(~combined_null_condition)
            dropped_rows = initial_rows - out_df.shape[0]
            logger.info(f"Dropped {dropped_rows} rows due to generated NaNs.")
        else:
            logger.info("No relevant lagged columns found to check for generated NaNs.")

    if drop_original_cols_after_lag:
        logger.info(f"Dropping original columns after lagging: {cols_to_lag}")
        out_df = out_df.drop(cols_to_lag)

    if restore_names:
        # Create a dictionary for renaming: {current_lagged_name: desired_original_name}
        renaming_dict = {f"{col}{suffix}": col for col in cols_to_lag}

        # Filter renaming_dict to only include columns that actually exist in the DataFrame
        actual_renaming_dict = {
            lagged_name: original_name
            for lagged_name, original_name in renaming_dict.items()
            if lagged_name in out_df.columns
        }

        if actual_renaming_dict:
            logger.info(
                f"Restoring original column names: {list(actual_renaming_dict.keys())} -> {list(actual_renaming_dict.values())}"
            )
            out_df = out_df.rename(actual_renaming_dict)
        else:
            logger.info("No lagged columns found to restore names for.")

    logger.info(f"Lag operation complete. Resulting shape: {out_df.shape}")
    return out_df


def create_macro_firm_interactions(
    df: pl.DataFrame,
    macro_columns: list[str],
    firm_columns: list[str],
    drop_macro_columns: bool,
) -> pl.DataFrame:
    """
    Creates interaction columns between macro characteristics and firm characteristics.
    Each interaction column is the product of a macro column and a firm column.

    Args:
        df: The input Polars DataFrame containing both macro and firm characteristics.
        macro_columns: A list of column names representing macro characteristics.
        firm_columns: A list of column names representing firm characteristics.
        drop_macro_columns: If True, the original macro columns will be dropped
                            from the output DataFrame after creating interactions.

    Returns:
        A new Polars DataFrame with the added interaction columns.
    """
    out_df = df.clone()
    interaction_expressions = []
    new_interaction_cols = []

    # Validate that all specified columns exist in the DataFrame
    missing_macro_cols = [c for c in macro_columns if c not in out_df.columns]
    if missing_macro_cols:
        raise ValueError(
            f"Macro columns specified for interaction not found in DataFrame: {missing_macro_cols}"
        )
    missing_firm_cols = [c for c in firm_columns if c not in out_df.columns]
    if missing_firm_cols:
        raise ValueError(
            f"Firm columns specified for interaction not found in DataFrame: {missing_firm_cols}"
        )

    logger.info(
        f"Creating interaction columns between macro: {macro_columns} and firm: {firm_columns}..."
    )

    for firm_col in firm_columns:
        for macro_col in macro_columns:
            interaction_col_name = f"{firm_col}_x_{macro_col}"
            new_interaction_cols.append(interaction_col_name)
            interaction_expressions.append(
                (pl.col(firm_col) * pl.col(macro_col)).alias(interaction_col_name)
            )

    if interaction_expressions:
        out_df = out_df.with_columns(interaction_expressions)
        logger.info(f"Created {len(new_interaction_cols)} new interaction columns.")
    else:
        logger.info("No interaction columns were created.")

    if drop_macro_columns:
        logger.info(f"Dropping original macro columns: {macro_columns}")
        out_df = out_df.drop(macro_columns)

    logger.info(f"Interaction creation complete. Resulting shape: {out_df.shape}")
    return out_df


def create_dummies(
    df: pl.DataFrame,
    column_to_dummy: str,
    drop_original_col: bool,
) -> pl.DataFrame:
    """
    Creates one-hot encoded (dummy) columns from a specified categorical column.

    Args:
        df: The input Polars DataFrame.
        column_to_dummy: The name of the column to convert to dummies.
        drop_original_col: If True, the original column is dropped from the output.

    Returns:
        A new Polars DataFrame with the added dummy columns.
    """
    if column_to_dummy not in df.columns:
        raise ValueError(
            f"Column '{column_to_dummy}' not found in DataFrame for dummy generation."
        )

    logger.info(f"Creating dummy variables for column '{column_to_dummy}'...")

    # Create a DataFrame containing only the one-hot encoded columns.
    # This is a robust way to handle it, as to_dummies works on a selection.
    dummy_df = df.select(pl.col(column_to_dummy)).to_dummies(drop_first=False)

    logger.info(f"Generated {dummy_df.shape[1]} new dummy columns.")

    # Concatenate the new dummy columns with the original DataFrame
    out_df = pl.concat([df, dummy_df], how="horizontal")

    if drop_original_col:
        logger.info(f"Dropping original column '{column_to_dummy}'.")
        out_df = out_df.drop(column_to_dummy)

    logger.info(f"Dummy generation complete. Resulting shape: {out_df.shape}")
    return out_df
